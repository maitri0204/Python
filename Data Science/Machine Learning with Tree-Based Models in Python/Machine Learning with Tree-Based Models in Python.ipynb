{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification-tree\n",
    "- Sequence of if-else questions about individual features.\n",
    "- infer class labels\n",
    "- able to capture non-linear relationships between features and labels\n",
    "- don't require feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification-tree in scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_size=0.2, random_state=42)\n",
    "df = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "print(rmse_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose Bias Problem\n",
    "- if f^ suffers high bias: CV error of f^ ~~ training set of error of f^ >> desired error.\n",
    "- f^ is said to underfit the training set. To remedy underfitting:\n",
    "    - increase model compexity\n",
    "    - for ex: increase max depth, decrease min samples per leaf,...\n",
    "    - gather more relevant features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold CV in sklearn on the Auto Dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Split data into 70% train and 30%test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate decision tree regressor and assign it ti 'dt'\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.14, random_state=42)\n",
    "\n",
    "# Evaluate the list of MSE ontained by 10-fold CV\n",
    "# Set n_jobs to -1 in order to exploit all CPU cores in computation\n",
    "MSE_CV = - cross_val_predict(dt, X_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit 'dt' to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of training set\n",
    "y_predict_train = dt.predict(X_train)\n",
    "\n",
    "# Predict the labels of test set\n",
    "y_predict_test = dt.predict(X_test) \n",
    "\n",
    "# CV MSE\n",
    "print('CV MSE: {:.2f}'.format(MSE_CV.mean())) \n",
    "\n",
    "# Training set MSE\n",
    "print('Train MSE: {:.2f}'.format(MSE(y_train, y_predict_train)))\n",
    "\n",
    "# Test set MSE\n",
    "print('Test MSE: {:.2f}'.format(MSE(y_test, y_predict_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of CARTs\n",
    "- Simple to understand\n",
    "- Simple to interpret\n",
    "- Easy to use\n",
    "- Flexibility: ability to describe non-linear dependencies\n",
    "- Preprocessing: no need to standardize features,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of CARTs\n",
    "- Classification: can only produce orthogonal decision boundaries.\n",
    "- Sensitive to small variations in the training set.\n",
    "- High variance: unconstrained CARTs may overfit the training set\n",
    "- Solution: ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "- Train different models on the same dataset\n",
    "- Let each model make its predictions\n",
    "- Meta-model: aggreagates predictions of individual models\n",
    "- Final prediction: more robust and less prone to errors.\n",
    "- Best results: models are skillful in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier in sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "                ('K Nearest Neighbours', knn),\n",
    "                ('Classification Tree', dt)]\n",
    "\n",
    "# Iterate over the defined list of tuples containing the classifiers\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Instance a VotingClassifier 'vc'\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "print('Voting Classifier: {.3f}'.format(accuracy_score(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Classifier in sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "\n",
    "# Fit 'bc' to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB Evaluation in sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "bc = BaggingClassifier(base_eliminator = dt, n_estimators=300, oob_score=True, n_jobs=-1)\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "print('Test set accuracy: {:.3f}'.format(test_accuracy))\n",
    "\n",
    "print('OOB accuracy: {:.3f}'.format(oob_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Diversity with Random Forests\n",
    "- Base estimator\n",
    "- Each estimator is trained on a different bootstrap sample having the same size as the training set\n",
    "- RF introduces further randomization in the training of individual trees\n",
    "- d features are sampled at each node without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests Regressor in sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MSE \n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=40, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance in sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n",
    "\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen');plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "- Stands for Adaptive Boosting\n",
    "- Each predictor pays more attention to the instances wrongly predicted by its predecessor.\n",
    "- Achieved by changing the weights of training instances\n",
    "- Each predictor is assigned a coefficient α\n",
    "- α depends on the predictor's training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classification in sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees\n",
    "- Sequential correction of predecessor's errors\n",
    "- Does not tweak the weights of training instances\n",
    "- Fit each predictor is trained using its predecessor's residual errors as labels\n",
    "- Gradient Boosted Trees: a CART is used as a base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting in sklearn\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MSE\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "gbt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred) ** (1/2)\n",
    "\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "- Each tree is trained on a random subset of rows of the training data\n",
    "- Sampled instance (40%-80% of the training set) are sampled without replacement\n",
    "- Features are sampled (without replacement) when choosing split points\n",
    "- Result: further ensemble diversity\n",
    "- Effect: adding further variance to the ensemble of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Bosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n",
    "\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgbt.predict(X_test) \n",
    "\n",
    "rmse_test = MSE(y_test, y_pred) ** (1/2)\n",
    "\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the hyperparameters of a CART in sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "print(dt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params_dt = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "    'max_features': [0.2, 0.4, 0.6, 0.8]\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring='acuuracy', cv=10, n_jobs=-1)\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print('Best hyperparameters: \\n', best_hyperparams)\n",
    "\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print('Best CV accuracy'.format(best_CV_score))\n",
    "\n",
    "best_model = grid_dt.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "print('Test set accuracy of best model: {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting RF Hyperparameters in sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "rf = RandomForestRegressor(random_state=SEED)\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_rf = {\n",
    "    'n_estimators': [300, 400, 500],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'min_samples_leaf': [0.1, 0.2],\n",
    "    'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=rf, param_grid=params_rf, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "print('Best hyperparameters: ', best_hyperparams)\n",
    "\n",
    "best_model = grid_rf.best_estimator_\n",
    "y_pred = best_model.predict(X_test) \n",
    "rmse_test = MSE(y_test, y_pred) ** (1/2)\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
